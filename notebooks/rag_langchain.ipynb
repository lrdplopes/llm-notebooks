{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNIYA+UpgjXylQ6R0XRvQx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lrdplopes/llm-notebooks/blob/main/notebooks/rag_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced RAG on HuggingFace documentation using LangChain ðŸ¤—\n",
        "\n",
        "Hands-on on how we can build an advanced RAG (Retrieval Augmented Generation) for answering a user's question about a specific knowledge base using LangChain.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">"
      ],
      "metadata": {
        "id": "6FmVwyav1izI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to install the required model dependencies.\n",
        "!pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap"
      ],
      "metadata": {
        "id": "nb3gt8nUZVwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will install the `dataset` library which includes the Hugging face dataset.\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "ar0SbApXcFLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option(\n",
        "    \"display.max_colwidth\", None\n",
        ")  # this will be helpful when visualizing retriever outputs"
      ],
      "metadata": {
        "id": "Ihzz2_oYdIRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `set_option `function is utilized herein to modify the `display.max_colwidth` configuration, which governs the maximum width of columns displayed when printing a DataFrame or Series to the console or Jupyter Notebook. Column width is measured in terms of character count."
      ],
      "metadata": {
        "id": "e5RhWCqIdX5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Knowledge-Base"
      ],
      "metadata": {
        "id": "CHvTkeoddlbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export your HF_TOKEN\n",
        "\n",
        "from google.colab import userdata\n",
        "userdata.get('<your_hf_token>')"
      ],
      "metadata": {
        "id": "5fdn1vh-ebE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
      ],
      "metadata": {
        "id": "oDaACA3UdOXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `load_dataset()` function accepts the argument `split=\"train\"` to indicate which part of the dataset should be loaded. Data sets used for machine learning are often split into multiple parts for model training, validation, and testing purposes."
      ],
      "metadata": {
        "id": "5YTQbsFTf7uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "    for doc in tqdm(ds)\n",
        "]"
      ],
      "metadata": {
        "id": "91aGN4fufbtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `RAW_KNOWLEDGE_BASE`: A list to store the documents in the KB.\n",
        "- `ds`: The loaded dataset of structured documents.\n",
        "- `tqdm`: A Python library that provides a progress bar for iterating over large datasets.\n",
        "- `LangchainDocument`: A class from the langchain library that represents a document in the KB.\n",
        "- `doc[\"text\"]`: The textual content of a document.\n",
        "- `doc[\"source\"]`: The metadata about the source of a document."
      ],
      "metadata": {
        "id": "kbFgyEEie56C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever Embeddings ðŸ“–"
      ],
      "metadata": {
        "id": "yzwwpkt7gHjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The Retriever's Function\n",
        "- **Analogy**: Imagine the Retriever as a search engine within a library. Instead of a large website, your \"library\" is the pre-structured knowledge base. Instead of a general search, the user is asking a specific question.\n",
        "- **Implementation**: The implementation typically involves transforming both questions and knowledge base documents into embeddings (vector representations) so that they can be efficiently compared.\n",
        "- **Goal**: The primary goal of the Retriever is to find the most relevant snippets in your knowledge base that can help answer the user's question.\n",
        "How is it done? Through a technique called \"Retrieval Embeddings\".\n",
        "\n",
        "2. What are \"Retrieval Embeddings\"\n",
        "- **Embeddings**: These are numerical representations of text. Think of them as \"digital signatures\" that capture the meaning and semantic essence of a sentence, paragraph, or even entire documents. Semantically similar texts are mapped to nearby points in vector space. This allows for efficient similarity-based searches.\n",
        "- How they are used\n",
        "  - **The Retriever**: Creates embeddings for its knowledge base. Generates an embedding from the user's question. Compares the question embedding with the knowledge base embeddings, finding the most similar ones (those closest in terms of meaning).\n",
        "\n",
        "3. Fine-Tuning the Search\n",
        "- `top_k`: This parameter determines how many \"snippets\" will be retrieved. In a learning context, we usually start with a smaller top_k value and adjust as needed.\n",
        "- `chunk size`: Refers to the length of each snippet. It's common to use varying sizes since some relevant sections may be shorter or longer.\n",
        "\n",
        "**Finding the Balance:**\n",
        "We need to balance a few things.\n",
        "\n",
        "- **Balancing**: Finding the right balance between top_k and chunk size is crucial. You want to ensure that the most informative snippets are retrieved without overwhelming the Reader Model with irrelevant information.\n",
        "- **Quality of Embeddings**: The quality of embeddings has a significant impact on the effectiveness of the retriever. More advanced embedding models can capture semantic nuances better, resulting in more accurate retrievals.\n",
        "\n",
        "4. Why the LangChain Library\n",
        "- **Flexibility**: LangChain makes it easier to work with different types of \"databases\" that store embeddings (the numerical representations of text).\n",
        "- **Metadata Preservation**: LangChain allows tracking relevant information about our snippets (like their original source), which can be very useful for our system."
      ],
      "metadata": {
        "id": "H2PkkIcHzVgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the Documents in `Chunks`\n",
        "\n",
        "### Let's explore the Recursive Chunking\n",
        "\n",
        "- Recursive Chunking: This is a technique that divides text into \"layers.\" It uses a list of \"separators\" (such as paragraph breaks `\\n\\n` line breaks `\\n` or sentence-ending periods `.`) to split texts hierarchically.\n",
        "- Adaptability: If one separator does not yield fragments of the ideal size, it applies the next separator to the obtained fragments, and so on.\n",
        "- Benefit: It helps preserve the general structure of the text, while still allowing some variation in the size of the chunks."
      ],
      "metadata": {
        "id": "ka5evSRE2QAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
        "# This list is taken from LangChain's MarkdownTextSplitter class.\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # the maximum number of characters in a chunk: we selected this value arbitrarily\n",
        "    chunk_overlap=100,  # the number of characters to overlap between chunks\n",
        "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
        "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
        "    separators=MARKDOWN_SEPARATORS,\n",
        ")\n",
        "docs_processed = []\n",
        "for doc in RAW_KNOWLEDGE_BASE:\n",
        "    docs_processed += text_splitter.split_documents([doc])"
      ],
      "metadata": {
        "id": "mWGhI4KefZCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> `max_seq_length`: Refers to the maximum number of tokens that the embedding model can process in a single sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "k0hkCbiwMHce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter.\n",
        "print(\n",
        "    f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\"\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "\n",
        "# Plot the distrubution of document lengths, counted as the number of tokens\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Msb_q_dE8-tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram analysis reveals that not all generated chunks adhere to the model's processing limit of 512 tokens. Consequently, some documents exceed this threshold, resulting in partial data loss due to model truncation.\n",
        "\n",
        "We may change the `RecursiveCharacterTextSplitter` class to count length in a number of tokens instead of a number of characters."
      ],
      "metadata": {
        "id": "TPx1kdA9ZjiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=MARKDOWN_SEPARATORS,\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "\n",
        "docs_processed = split_documents(\n",
        "    512,  # We choose a chunk size adapted to our model\n",
        "    RAW_KNOWLEDGE_BASE,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")\n",
        "\n",
        "# Let's visualize the chunk sizes we would have in tokens from a common model\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KJwHUpQPMaxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Vector Database\n",
        "\n",
        "To build the vector database, it's necessary to compute the embeddings for each piece of knowledge, transforming the text into information that machine learning models can process. These embeddings are then stored in a vector database, which serves as a library for finding relevant information.\n",
        "\n",
        "When a user makes a query, it is converted into an embedding and a `nearest-neighbor` search algorithm finds the most similar chunks. This process needs to be fast and efficient, for which the `FAISS` library is used.\n",
        "\n",
        "The next step involves turning this concept into code, generating the embeddings, normalizing them, building the database with `FAISS` and writing the logic for the user's query."
      ],
      "metadata": {
        "id": "qi0c16vEdRso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n",
        ")\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        ")"
      ],
      "metadata": {
        "id": "lmuqNGH7ZyV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `PaCMAP`"
      ],
      "metadata": {
        "id": "OqxRnYD1Z5YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embed a user query in the same space\n",
        "user_query = \"How to create a pipeline object?\"\n",
        "query_vector = embedding_model.embed_query(user_query)"
      ],
      "metadata": {
        "id": "jBq4wLJ9K3hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pacmap\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "embedding_projector = pacmap.PaCMAP(\n",
        "    n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1\n",
        ")\n",
        "\n",
        "embeddings_2d = [\n",
        "    list(KNOWLEDGE_VECTOR_DATABASE.index.reconstruct_n(idx, 1)[0])\n",
        "    for idx in range(len(docs_processed))\n",
        "] + [query_vector]\n",
        "\n",
        "# fit the data (The index of transformed data corresponds to the index of the original data)\n",
        "documents_projected = embedding_projector.fit_transform(\n",
        "    np.array(embeddings_2d), init=\"pca\"\n",
        ")"
      ],
      "metadata": {
        "id": "i4IC4wioZ9mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict(\n",
        "    [\n",
        "        {\n",
        "            \"x\": documents_projected[i, 0],\n",
        "            \"y\": documents_projected[i, 1],\n",
        "            \"source\": docs_processed[i].metadata[\"source\"].split(\"/\")[1],\n",
        "            \"extract\": docs_processed[i].page_content[:100] + \"...\",\n",
        "            \"symbol\": \"circle\",\n",
        "            \"size_col\": 4,\n",
        "        }\n",
        "        for i in range(len(docs_processed))\n",
        "    ]\n",
        "    + [\n",
        "        {\n",
        "            \"x\": documents_projected[-1, 0],\n",
        "            \"y\": documents_projected[-1, 1],\n",
        "            \"source\": \"User query\",\n",
        "            \"extract\": user_query,\n",
        "            \"size_col\": 100,\n",
        "            \"symbol\": \"star\",\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# visualize the embedding\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x=\"x\",\n",
        "    y=\"y\",\n",
        "    color=\"source\",\n",
        "    hover_data=\"extract\",\n",
        "    size=\"size_col\",\n",
        "    symbol=\"symbol\",\n",
        "    color_discrete_map={\"User query\": \"black\"},\n",
        "    width=1000,\n",
        "    height=700,\n",
        ")\n",
        "fig.update_traces(\n",
        "    marker=dict(opacity=1, line=dict(width=0, color=\"DarkSlateGrey\")),\n",
        "    selector=dict(mode=\"markers\"),\n",
        ")\n",
        "fig.update_layout(\n",
        "    legend_title_text=\"<b>Chunk source</b>\",\n",
        "    title=\"<b>2D Projection of Chunk Embeddings via PaCMAP</b>\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "g5p6NhSfaEzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
        "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
        "print(\n",
        "    \"\\n==================================Top document==================================\"\n",
        ")\n",
        "print(retrieved_docs[0].page_content)\n",
        "print(\"==================================Metadata==================================\")\n",
        "print(retrieved_docs[0].metadata)"
      ],
      "metadata": {
        "id": "QcD01xetaVLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reader LLM\n",
        "\n",
        "## Reader Model\n",
        "\n",
        "The importance of choosing an LLM with adequate natural language understanding capabilities is emphasized, focusing on its ability to process the entire context (including user queries and retrieved chunks) without truncating information.\n",
        "\n",
        "Key considerations include the model's **maximum sequence length** (to ensure all context is processed), computational efficiency, and the balance between model size and capability to generate accurate responses.\n",
        "\n",
        "The chosen model is `HuggingFaceH4/zephyr-7b-beta` for its long-sequence processing capability and cost-effectiveness. **Optimization** through quantization is discussed to reduce model size and increase inference speed, using the `BitsAndBytesConfig` in the Hugging Face library.\n",
        "\n",
        "The text concludes with code snippets for configuring and loading the quantized model using Hugging Face's `pipeline`, `tokenizer`, and `AutoModelForCausalLM`, aiming to facilitate efficient text generation for the reader model.\n",
        "\n",
        "Continuous monitoring of new models and performance benchmarking are recommended to ensure the chosen model remains optimal for the project's needs."
      ],
      "metadata": {
        "id": "HKMgmno8aF9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    READER_MODEL_NAME, quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        ")"
      ],
      "metadata": {
        "id": "V_IVO3PiaImV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "READER_LLM(\"What is 4+4? Answer:\")"
      ],
      "metadata": {
        "id": "CfCPeV0CDA6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt\n",
        "\n",
        "The RAG prompt template below is what we will feed to the Reader LLM."
      ],
      "metadata": {
        "id": "HQvZHYeWO4TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "print(RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "VgpnMQ0UPGn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's test it."
      ],
      "metadata": {
        "id": "qAkXC5IQZBBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_text = [\n",
        "    doc.page_content for doc in retrieved_docs\n",
        "]  # we only need the text of the documents\n",
        "context = \"\\nExtracted documents:\\n\"\n",
        "context += \"\".join(\n",
        "    [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
        ")\n",
        "\n",
        "final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
        "    question=\"How to create a pipeline object?\", context=context\n",
        ")\n",
        "\n",
        "# Redact an answer\n",
        "answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "nuW9VSFgYiLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reranking\n",
        "\n",
        "The reranking is crucial for refining the results of an initial search, promoting the most relevant documents and improving the quality of the responses. The `Colbertv2`, a cross-encoder, is useful for reranking due to its detailed analysis of the relevance between the question and the document, resulting in a more accurate ranking. The `RAGatouille` library simplifies the integration of `Colbertv2`."
      ],
      "metadata": {
        "id": "AJEyAJmggFL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
      ],
      "metadata": {
        "id": "P5LbvLAtXdGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assembling"
      ],
      "metadata": {
        "id": "CEHvOm8iXgo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Pipeline\n",
        "\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: Pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs"
      ],
      "metadata": {
        "id": "73Ei-N6NXk2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"how to create a pipeline object?\"\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(\n",
        "    question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER\n",
        ")"
      ],
      "metadata": {
        "id": "TQbbahKJXpD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n",
        "print(\"==================================Source docs==================================\")\n",
        "for i, doc in enumerate(relevant_docs):\n",
        "    print(f\"Document {i}------------------------------------------------------------\")\n",
        "    print(doc)"
      ],
      "metadata": {
        "id": "-GPdmQ5nXrRX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}