{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVbikVJBHoUoFhugPWRPM9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lrdplopes/llm-notebooks/blob/main/notebooks/rag_youtube.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-dotenv"
      ],
      "metadata": {
        "id": "zRg8kCHCLHeC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd4ojymYLFbU",
        "outputId": "b5cad023-5e64-4cd0-b768-175377eed2eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# This is the YouTube video we're going to use.\n",
        "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\""
      ],
      "metadata": {
        "id": "rmSuLel4Sumh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the model\n",
        "Define the LLM that will be part of the workflow"
      ],
      "metadata": {
        "id": "1mKV2i85S_Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain openai langchain_openai"
      ],
      "metadata": {
        "id": "RaKydN5rTJDp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "BZUAoaD9S3lo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model"
      ],
      "metadata": {
        "id": "78VG70bGTgGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq-AMrpPTV2m",
        "outputId": "15a41ae0-1910-472f-b0f3-6e40ba5b8f06"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The Los Angeles Dodgers won the 2020 World Series during the COVID-19 pandemic.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 21, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fda93848-2300-408b-b259-5f536ad8458b-0')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transforming it in `STR`"
      ],
      "metadata": {
        "id": "6dM4z-kxUrqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = model | parser\n",
        "chain.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T4GHdMuZTsqr",
        "outputId": "739a8b85-b58c-4776-d411-883c088299a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Los Angeles Dodgers won the World Series during the COVID-19 pandemic in 2020. They defeated the Tampa Bay Rays in a six-game series to claim their first championship since 1988.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing `prompt` templates"
      ],
      "metadata": {
        "id": "LiAWGOL6VCo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "template = \"\"\"\n",
        "Answer the question based on the context below. If you can't\n",
        "answer the question, reply \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt.format(context=\"Lucas' is Gabriella's husband\", question=\"Who is Lucas?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7TCgm11AU644",
        "outputId": "93d4d0d8-df16-4597-aa38-b9b796d1dd87"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: \\nAnswer the question based on the context below. If you can\\'t\\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Lucas\\' is Gabriella\\'s husband\\nQuestion: Who is Lucas?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | parser\n",
        "chain.invoke({\n",
        "    \"question\": \"Who is Lucas?\",\n",
        "    \"context\": \"Lucas' is Gabriella's husband\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VCR4ex6eVrLX",
        "outputId": "b62878e9-cbce-4a62-98dd-4d5fe4ef2f23"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Lucas is Gabriella's husband.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining `Chains`\n",
        "We can combine different chains to create more complex workflows. For example, let's create a second chain that translates the answer from the first chain into a different language.\n",
        "\n",
        "Let's start by creating a new prompt template for the translation chain."
      ],
      "metadata": {
        "id": "B2lK6NTPWbeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a `translation` chain\n",
        "translation_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the {answer} to the {language}\"\n",
        ")"
      ],
      "metadata": {
        "id": "OJIWdDKUWYEs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "translation_chain = (\n",
        "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
        ")\n",
        "\n",
        "translation_chain.invoke(\n",
        "    {\n",
        "        \"context\": \"Lucas' is Gabriella's husband\",\n",
        "        \"question\": \"Who is Lucas?\",\n",
        "        \"language\": \"French\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8GV-MH5QILr3",
        "outputId": "58baf18f-540b-4f40-df9a-9609406b7d62"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lucas est le mari de Gabriella.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribing the YouTube Video\n",
        "The context we want to send the model comes from a YouTube video. Let's download the video and transcribe it using OpenAI's Whisper - https://openai.com/research/whisper"
      ],
      "metadata": {
        "id": "ueHRP5fkJUOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q whisper pytube openai-whisper"
      ],
      "metadata": {
        "id": "7mkB4Ut4K0LK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import whisper\n",
        "from pytube import YouTube\n",
        "\n",
        "# create an `if` only if we haven't created the transcription file yet.\n",
        "if not os.path.exists(\"transcription.txt\"):\n",
        "    youtube = YouTube(YOUTUBE_VIDEO)\n",
        "    audio = youtube.streams.filter(only_audio=True).first()\n",
        "\n",
        "    # load the `base` model\n",
        "    whisper_model = whisper.load_model(\"base\")\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        file = audio.download(output_path=tmpdir)\n",
        "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
        "        with open(\"transcription.txt\", \"w\") as file:\n",
        "            file.write(transcription)"
      ],
      "metadata": {
        "id": "NWKF6VwAIvit"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's read the transcription and display the first few characters to ensure everything works as expected."
      ],
      "metadata": {
        "id": "ZC2e9w6RPuaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"transcription.txt\", \"r\") as file:\n",
        "    transcription = file.read()\n",
        "\n",
        "transcription[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BPsaeq8FKx7w",
        "outputId": "cce1887d-990a-451a-d538-a6c68d414cde"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the entire transcription as context\n",
        "If we try to invoke the chain using the transcription as context, the model will return an error because the context is too long.\n",
        "\n",
        "Large Language Models support limitted context sizes. The video we are using is too long for the model to handle, so we need to find a different solution."
      ],
      "metadata": {
        "id": "XC8iakZmQCkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  chain.invoke({\n",
        "      \"context\": transcription,\n",
        "      \"question\": \"Is reading papers a good idea?\"\n",
        "  })\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBgjUNCrP7tp",
        "outputId": "200af9ae-87fd-4d8e-ba60-6f57a773338d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 47047 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the transcription\n",
        "Since we can't use the entire transcription as the context for the model, a potential solution is to split the transcription into smaller chunks. We can then invoke the model using only the relevant chunks to answer a particular question.\n",
        "\n",
        "- Let's start loading the transcription in `memory`"
      ],
      "metadata": {
        "id": "iVChpju6QZjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community"
      ],
      "metadata": {
        "id": "eWYjgY-rQ4h-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"transcription.txt\")\n",
        "text_documents = loader.load()"
      ],
      "metadata": {
        "id": "WJAiV3DwQd20"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many different ways to split a document. For this example, we'll use a simple splitter that splits the document into chunks of a fixed size - https://python.langchain.com/docs/modules/data_connection/document_transformers/."
      ],
      "metadata": {
        "id": "quB5_fbaRTnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        ")\n",
        "documents = text_splitter.split_documents(text_documents)"
      ],
      "metadata": {
        "id": "e10Ag37dQ1pn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the relevant chunks\n",
        "Given a particular question, we must find the relevant chunks from the transcription to send to the model. Here is where the idea of `embeddings` comes in - https://dashboard.cohere.com/playground/embed."
      ],
      "metadata": {
        "id": "BAeCJm9SSF7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "embedded_query = embeddings.embed_query(\"Who is Gabriella\")\n",
        "\n",
        "print(f\"Embedding lenght: {len(embedded_query)}\")\n",
        "print(embedded_query[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2ItsQtJRrF9",
        "outputId": "729734f5-421d-414b-c6d4-22159279b563"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding lenght: 1536\n",
            "[-0.013011531379663712, 0.0048802254384083185, -0.022562932928235913, -0.01819451768934817, -0.001633649975826097, -0.0026779964937517775, -0.0131773291548687, -0.005561438562932889, -0.008289895279507574, -0.0315736877355085]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_1 = embeddings.embed_query(\"Lucas' hobbie is read\")\n",
        "sentence_2 = embeddings.embed_query(\"Gabriella' hobbie is stay in the phone\")\n"
      ],
      "metadata": {
        "id": "js6Jky-sSx8M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now compute the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
        "\n",
        "We can use Cosine Similarity to calculate the similarity between the query and each of the sentences - https://en.wikipedia.org/wiki/Cosine_similarity."
      ],
      "metadata": {
        "id": "Vzz68xEJT_EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query_sentence_1 = cosine_similarity([embedded_query], [sentence_1])[0][0]\n",
        "query_sentence_2 = cosine_similarity([embedded_query], [sentence_2])[0][0]\n",
        "\n",
        "query_sentence_1, query_sentence_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD6IVXYnT6up",
        "outputId": "e81a0397-40b7-4b95-e7c6-d5ee3116cd74"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7365613597765673, 0.8350747678086314)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up a Vector Store\n",
        "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a vector store. A vector store is a database of embeddings that specializes in fast similarity searches.\n",
        "\n",
        "To understand how it works, first let's create the `vector-store` in memory."
      ],
      "metadata": {
        "id": "UPcN06ggUo2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  \"docarray\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ9z6juxVjOO",
        "outputId": "9027be50-2af9-486f-d216-a09ab3b08897"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/270.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m225.3/270.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.2/270.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"Mary's sister is Susana\",\n",
        "        \"John and Tommy are brothers\",\n",
        "        \"Patricia likes white cars\",\n",
        "        \"Pedro's mother is a teacher\",\n",
        "        \"Lucia drives an Audi\",\n",
        "        \"Mary has two siblings\",\n",
        "    ],\n",
        "    embedding=embeddings,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LvvWg9GUUhl",
        "outputId": "b5cbe574-15fa-45ff-d765-9ff11ac8bd9a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
            "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore1.similarity_search_with_score(\"Who likes Audi\", k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De2hYjrwVIh-",
        "outputId": "4282cdca-7b20-4e7d-cfc9-ed032cb9e174"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(page_content='Lucia drives an Audi'), 0.858028343515171),\n",
              " (Document(page_content='Patricia likes white cars'), 0.8273385734903588),\n",
              " (Document(page_content='Mary has two siblings'), 0.7164998143349375)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting the vector store to the chain\n",
        "We can use the vector store to find the most relevant chunks from the transcription to send to the model.\n",
        "\n",
        "We need to configure a Retriever. The retriever will run a similarity search in the vector store and return the most similar documents back to the next step in the chain - https://python.langchain.com/docs/modules/data_connection/retrievers/."
      ],
      "metadata": {
        "id": "5PbdHEqKXnTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_1 = vectorstore1.as_retriever()\n",
        "retriever_1.invoke(\"Who likes Audi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haEGuRGyXcVL",
        "outputId": "390d38c4-3f72-4e03-efbb-c3ed69e09ce1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Lucia drives an Audi'),\n",
              " Document(page_content='Patricia likes white cars'),\n",
              " Document(page_content='Mary has two siblings'),\n",
              " Document(page_content=\"Pedro's mother is a teacher\")]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our prompt expects two parameters, `\"context\"` and `\"question\"`. We can use the retriever to find the chunks we'll use as the context to answer the question.\n",
        "\n",
        "We can create a map with the two inputs by using the `RunnableParallel` and `RunnablePassthrough` classes. This will allow us to pass the context and question to the prompt as a map with the keys \"context\" and \"question.\""
      ],
      "metadata": {
        "id": "YGVlXW3ZYGmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "setup = RunnableParallel(context=retriever_1, question=RunnablePassthrough())\n",
        "setup.invoke(\"What color is Lucas car?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKK70OtfX3Nd",
        "outputId": "283ac954-15c5-4b6d-a201-ffb4fb45ae70"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(page_content='Lucia drives an Audi'),\n",
              "  Document(page_content='Patricia likes white cars'),\n",
              "  Document(page_content='John and Tommy are brothers'),\n",
              "  Document(page_content=\"Mary's sister is Susana\")],\n",
              " 'question': 'What color is Lucas car?'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = setup | prompt | model | parser\n",
        "chain.invoke(\"What color is Lucas car?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ck-3eyWMYowE",
        "outputId": "894c29a0-9435-4b2f-e7de-eaceaa21d0b2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading transcription into the vector store\n",
        "We initialized the vector store with a few random strings. Let's create a new vector store using the chunks from the video transcription."
      ],
      "metadata": {
        "id": "IgoHw3FKY9if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore_2 = DocArrayInMemorySearch.from_documents(\n",
        "    documents,\n",
        "    embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "bt4-vfV1Y2O5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": vectorstore_2.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"What is the essential message from the video?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WTJU0iVTZJON",
        "outputId": "63282192-896b-42e7-bb0c-4147e65d6d58"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The essential message from the video is about the importance of collecting ground truth data for training neural networks, different mechanisms for collecting training data such as human annotation and simulation, and the significance of incremental product improvement and generating revenue along the way.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bmhvqDvyZjMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}