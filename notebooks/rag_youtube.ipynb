{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjwrsp7CYmZgZOeW6XjCi8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-dotenv"
      ],
      "metadata": {
        "id": "zRg8kCHCLHeC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd4ojymYLFbU",
        "outputId": "b5cad023-5e64-4cd0-b768-175377eed2eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# This is the YouTube video we're going to use.\n",
        "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\""
      ],
      "metadata": {
        "id": "rmSuLel4Sumh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the model\n",
        "Define the LLM that will be part of the workflow"
      ],
      "metadata": {
        "id": "1mKV2i85S_Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain openai langchain_openai"
      ],
      "metadata": {
        "id": "RaKydN5rTJDp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "BZUAoaD9S3lo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model"
      ],
      "metadata": {
        "id": "78VG70bGTgGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"Which NBA team won the championship in 2021?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq-AMrpPTV2m",
        "outputId": "57248221-b58f-42dd-986a-7e78f821259b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The Milwaukee Bucks won the NBA championship in 2021.', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 18, 'total_tokens': 30}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f0c1f90e-2672-4ef1-ba19-724caf83485b-0')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transforming it in `STR`"
      ],
      "metadata": {
        "id": "6dM4z-kxUrqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = model | parser\n",
        "chain.invoke(\"Which NBA team won the championship in 2021?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T4GHdMuZTsqr",
        "outputId": "c3d9381f-0a1f-4fd6-cfbc-bbbd0925a823"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Milwaukee Bucks won the NBA championship in 2021.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing `prompt` templates"
      ],
      "metadata": {
        "id": "LiAWGOL6VCo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "template = \"\"\"\n",
        "Answer the question based on the context below. If you can't\n",
        "answer the question, reply \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt.format(context=\"The Great Wall of China is one of the Seven Wonders of the Medieval World\", question=\"What is one of the Seven Wonders of the Medieval World?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7TCgm11AU644",
        "outputId": "64216a2b-2ecb-4efc-9ee9-6fbf4ca14922"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: \\nAnswer the question based on the context below. If you can\\'t\\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: The Great Wall of China is one of the Seven Wonders of the Medieval World\\nQuestion: What is one of the Seven Wonders of the Medieval World?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | parser\n",
        "chain.invoke({\n",
        "    \"question\": \"What is one of the Seven Wonders of the Medieval World?\",\n",
        "    \"context\": \"The Great Wall of China is one of the Seven Wonders of the Medieval World\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VCR4ex6eVrLX",
        "outputId": "a52e944c-74ee-4895-b7be-dce130a27352"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Great Wall of China'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining `Chains`\n",
        "We can combine different chains to create more complex workflows. For example, let's create a second chain that translates the answer from the first chain into a different language.\n",
        "\n",
        "Let's start by creating a new prompt template for the translation chain."
      ],
      "metadata": {
        "id": "B2lK6NTPWbeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a `translation` chain\n",
        "translation_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the {answer} to the {language}\"\n",
        ")"
      ],
      "metadata": {
        "id": "OJIWdDKUWYEs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "translation_chain = (\n",
        "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
        ")\n",
        "\n",
        "translation_chain.invoke(\n",
        "    {\n",
        "        \"context\": \"The Great Wall of China is one of the Seven Wonders of the Medieval World\",\n",
        "        \"question\": \"What is one of the Seven Wonders of the Medieval World?\",\n",
        "        \"language\": \"Chinese\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8GV-MH5QILr3",
        "outputId": "36a6bed1-1ee5-4cb5-cdd5-0989d09261e2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'中国长城 (Zhōngguó Chángchéng)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribing the YouTube Video\n",
        "The context we want to send the model comes from a YouTube video. Let's download the video and transcribe it using OpenAI's Whisper - https://openai.com/research/whisper"
      ],
      "metadata": {
        "id": "ueHRP5fkJUOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q whisper pytube openai-whisper"
      ],
      "metadata": {
        "id": "7mkB4Ut4K0LK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import whisper\n",
        "from pytube import YouTube\n",
        "\n",
        "# create an `if` only if we haven't created the transcription file yet.\n",
        "if not os.path.exists(\"transcription.txt\"):\n",
        "    youtube = YouTube(YOUTUBE_VIDEO)\n",
        "    audio = youtube.streams.filter(only_audio=True).first()\n",
        "\n",
        "    # load the `base` model\n",
        "    whisper_model = whisper.load_model(\"base\")\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        file = audio.download(output_path=tmpdir)\n",
        "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
        "        with open(\"transcription.txt\", \"w\") as file:\n",
        "            file.write(transcription)"
      ],
      "metadata": {
        "id": "NWKF6VwAIvit"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's read the transcription and display the first few characters to ensure everything works as expected."
      ],
      "metadata": {
        "id": "ZC2e9w6RPuaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"transcription.txt\", \"r\") as file:\n",
        "    transcription = file.read()\n",
        "\n",
        "transcription[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BPsaeq8FKx7w",
        "outputId": "cce1887d-990a-451a-d538-a6c68d414cde"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the entire transcription as context\n",
        "If we try to invoke the chain using the transcription as context, the model will return an error because the context is too long.\n",
        "\n",
        "Large Language Models support limitted context sizes. The video we are using is too long for the model to handle, so we need to find a different solution."
      ],
      "metadata": {
        "id": "XC8iakZmQCkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  chain.invoke({\n",
        "      \"context\": transcription,\n",
        "      \"question\": \"Is reading papers a good idea?\"\n",
        "  })\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBgjUNCrP7tp",
        "outputId": "200af9ae-87fd-4d8e-ba60-6f57a773338d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 47047 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the transcription\n",
        "Since we can't use the entire transcription as the context for the model, a potential solution is to split the transcription into smaller chunks. We can then invoke the model using only the relevant chunks to answer a particular question.\n",
        "\n",
        "- Let's start loading the transcription in `memory`"
      ],
      "metadata": {
        "id": "iVChpju6QZjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community"
      ],
      "metadata": {
        "id": "eWYjgY-rQ4h-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"transcription.txt\")\n",
        "text_documents = loader.load()"
      ],
      "metadata": {
        "id": "WJAiV3DwQd20"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many different ways to split a document - https://python.langchain.com/docs/modules/data_connection/document_transformers/."
      ],
      "metadata": {
        "id": "quB5_fbaRTnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        ")\n",
        "documents = text_splitter.split_documents(text_documents)"
      ],
      "metadata": {
        "id": "e10Ag37dQ1pn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the relevant chunks\n",
        "Given a particular question, we must find the relevant chunks from the transcription to send to the model. Here is where the idea of `embeddings` comes in - https://dashboard.cohere.com/playground/embed."
      ],
      "metadata": {
        "id": "BAeCJm9SSF7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "embedded_query = embeddings.embed_query(\"What is one of the Seven Wonders of the Medieval World?\")\n",
        "\n",
        "print(f\"Embedding lenght: {len(embedded_query)}\")\n",
        "print(embedded_query[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2ItsQtJRrF9",
        "outputId": "b96384e0-5e88-4ba8-e127-7bba5b6f263d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding lenght: 1536\n",
            "[0.014555458821796902, -0.010473769094106803, 0.007913086514982138, -0.0074895154227005364, -0.006648790050637212, -0.008824407069092396, -0.020305763805968755, -0.03126728143058164, 0.002188452971761366, -0.030651178023626584]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_1 = embeddings.embed_query(\"John's favorite sport is basketball\")\n",
        "sentence_2 = embeddings.embed_query(\"Emma's favorite food is sushi\")\n"
      ],
      "metadata": {
        "id": "js6Jky-sSx8M"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now compute the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
        "\n",
        "We can use Cosine Similarity to calculate the similarity between the query and each of the sentences - https://en.wikipedia.org/wiki/Cosine_similarity."
      ],
      "metadata": {
        "id": "Vzz68xEJT_EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query_sentence_1 = cosine_similarity([embedded_query], [sentence_1])[0][0]\n",
        "query_sentence_2 = cosine_similarity([embedded_query], [sentence_2])[0][0]\n",
        "\n",
        "query_sentence_1, query_sentence_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD6IVXYnT6up",
        "outputId": "793321c2-19d2-4d1a-8695-65f7c34f7cef"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7232313849506155, 0.6990840014188955)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up a Vector Store\n",
        "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a vector store. A vector store is a database of embeddings that specializes in fast similarity searches.\n",
        "\n",
        "To understand how it works, first let's create the `vector-store` in memory."
      ],
      "metadata": {
        "id": "UPcN06ggUo2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  \"docarray\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ9z6juxVjOO",
        "outputId": "9027be50-2af9-486f-d216-a09ab3b08897"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/270.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m225.3/270.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.2/270.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"Anna's brother is Michael\",\n",
        "        \"David and Sarah are twins\",\n",
        "        \"Lisa prefers red apples\",\n",
        "        \"Mark's father is a doctor\",\n",
        "        \"Sophia rides a bicycle\",\n",
        "        \"Emily owns a bookstore\",\n",
        "    ],\n",
        "    embedding=embeddings,\n",
        ")"
      ],
      "metadata": {
        "id": "5LvvWg9GUUhl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore1.similarity_search_with_score(\"Who likes bicycle\", k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De2hYjrwVIh-",
        "outputId": "a6374c45-6fb8-4a39-9d25-89df11b4ccaf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(page_content='Sophia rides a bicycle'), 0.8466580439383807),\n",
              " (Document(page_content='Lisa prefers red apples'), 0.7709716918314516),\n",
              " (Document(page_content='Emily owns a bookstore'), 0.7585758891421106)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting the vector store to the chain\n",
        "We can use the vector store to find the most relevant chunks from the transcription to send to the model.\n",
        "\n",
        "We need to configure a Retriever. The retriever will run a similarity search in the vector store and return the most similar documents back to the next step in the chain - https://python.langchain.com/docs/modules/data_connection/retrievers/."
      ],
      "metadata": {
        "id": "5PbdHEqKXnTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_1 = vectorstore1.as_retriever()\n",
        "retriever_1.invoke(\"What fruits does Lisa prefer?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haEGuRGyXcVL",
        "outputId": "a38ef452-3d8d-48eb-d1d8-7eb4f72cc196"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Lisa prefers red apples'),\n",
              " Document(page_content='Emily owns a bookstore'),\n",
              " Document(page_content=\"Anna's brother is Michael\"),\n",
              " Document(page_content='Sophia rides a bicycle')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our prompt expects two parameters, `\"context\"` and `\"question\"`. We can use the retriever to find the chunks we'll use as the context to answer the question.\n",
        "\n",
        "We can create a map with the two inputs by using the `RunnableParallel` and `RunnablePassthrough` classes. This will allow us to pass the context and question to the prompt as a map with the keys \"context\" and \"question.\""
      ],
      "metadata": {
        "id": "YGVlXW3ZYGmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "setup = RunnableParallel(context=retriever_1, question=RunnablePassthrough())\n",
        "setup.invoke(\"What fruits does Lisa prefer?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKK70OtfX3Nd",
        "outputId": "0f07c487-4937-4c54-9050-597c9a561467"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(page_content='Lisa prefers red apples'),\n",
              "  Document(page_content='Emily owns a bookstore'),\n",
              "  Document(page_content=\"Anna's brother is Michael\"),\n",
              "  Document(page_content='Sophia rides a bicycle')],\n",
              " 'question': 'What fruits does Lisa prefer?'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = setup | prompt | model | parser\n",
        "chain.invoke(\"What fruits does Lisa prefer?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ck-3eyWMYowE",
        "outputId": "23a3bcf9-cb28-4706-c7f5-2020ad41f22c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading transcription into the vector store\n",
        "We initialized the vector store with a few random strings. Let's create a new vector store using the chunks from the video transcription."
      ],
      "metadata": {
        "id": "IgoHw3FKY9if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore_2 = DocArrayInMemorySearch.from_documents(\n",
        "    documents,\n",
        "    embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "bt4-vfV1Y2O5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": vectorstore_2.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"What is the essential message from the video?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WTJU0iVTZJON",
        "outputId": "63282192-896b-42e7-bb0c-4147e65d6d58"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The essential message from the video is about the importance of collecting ground truth data for training neural networks, different mechanisms for collecting training data such as human annotation and simulation, and the significance of incremental product improvement and generating revenue along the way.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bmhvqDvyZjMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}